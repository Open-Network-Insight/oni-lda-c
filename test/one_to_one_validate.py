"""
Validation program for the oni-lda-c one_to_one test case.

Usage:
python2.6 one_to_one_validate.py -i <path to input file> -t <topic count> -e <floating point error threshold>

path to input file: the path to the LDA output file generated by one_to_one.sh
topic_count : number of topics used to generate the one-to-one test case
floating point error threshold : value of epsilon used to decree values "close enough" to 0 or 1 in the LDA output
"""

import sys
import argparse
import math

"""
Helper functions.
"""

# determine the number of elements in a list that are within epsilon of 0
def near_zero_count(collection, epsilon):
    return len([x for x in collection if math.fabs(x) < epsilon])

# determine the number of elements in a list that are within epsilon of 1
def near_one_count(collection, epsilon):
    return len([x for x in collection if math.fabs(1 - x) < epsilon])

# determine if a vector is close to a unit: exactly one coordinate near 1 and the rest near 0
def vector_near_unit(collection, epsilon):
    return (near_one_count(collection, epsilon) == 1 or near_zero_count(collection, epsilon) == (len(collection) - 1))


# normalize a list of numbers so that the sum is one
# assume that the sum of the incoming list is non-zero
def normalize(l):
    s = sum(l)
    return map(lambda x: float(x) / s, l)


# Find the set of elements in a collection that satisfy a predicate.
def set_of_satisfiers(predicate, collection):
    return set(filter(predicate, collection))

# Determine if a collection contains an item that satisfies a predicate.
def exists(predicate, collection):
    return len(filter(predicate, collection)) > 0

# Find the indices of elements in a list that satisfy a given predicate.
def indices_of_satisfiers(predicate, list):
    return set_of_satisfiers(lambda i: predicate(list[i]), range(0, len(list)))

# Take the union of all sets in a collection of sets.
def union(collection_of_sets):
    return reduce(lambda s, t: s.union(t), collection_of_sets)


"""
Validate that the "beta" file generated by LDA is correct.
The beta file is the file of word-mixes per topic.
In the one word per topic, one topic per document scenario w we expect a text file:
- one line per topic
- each line consists of num_words many white-space separated floating point numbers
- the rows encode log proabilities
- each row is an unit vector (ie. all probability is on exactly one word)
- no two rows assign non-zero probability to the same word
"""
def test_beta(test_dir, num_topics, num_words):
    beta_path = test_dir + '/' + 'final.beta'
    f = open(beta_path, 'r')

    lines = [l.strip() for l in f.readlines()]

    test_passed = True

    if (len(lines) != num_topics):
        print "TEST FAILED: " + beta_path + " should contain " + str(num_topics) + " lines, one per topic."
        test_passed = False
    else:
        words_per_topic = [[math.exp(float(entry)) for entry in line.split(' ')] for line in lines]

        if (exists(lambda word_mix: len(word_mix) != num_words, words_per_topic)):

            test_passed = False
            print "TEST FAILED: all rows of " + beta_path + " should contain " + str(num_words) + " many entries."
            bad_idx = indices_of_satisfiers(lambda word_mix: len(word_mix) != num_words, words_per_topic).pop()
            example = lines[bad_idx]
            print "Example bad row: " + str(example)

        else:
            if (exists(lambda word_mix: not vector_near_unit(word_mix, epsilon), words_per_topic)):
                test_passed = False
                print "TEST FAILED: all rows of " + beta_path + " should coutain exactly one entry within " \
                      + str(epsilon) + " of 1 and exactly " + str(num_words - 1) + \
                      " manys entries within " + str(epsilon) + " of 0.0"
                bad_idx = indices_of_satisfiers(lambda word_mix: not vector_near_unit(word_mix, epsilon),
                                                words_per_topic).pop()
                print "Example bad probability vector: " + str(words_per_topic[bad_idx])
                print "From raw row: " + str(lines[bad_idx])
            else:
                words_in_topics = union(
                    [indices_of_satisfiers(lambda x: math.fabs(1 - x) < epsilon, tmix) for tmix in words_per_topic])
                if (len(words_in_topics) != num_words):
                    test_passed = False
                    print "TEST FAILED:  Different topics should have their topic mixes centered on different words."
    f.close()
    return test_passed

"""
Validate that the "gamma" file generated by LDA is correct.
The gamma file is the file of topic-mixes per document.
In the one word per topic, one topic per document scenario w we expect a text file:
- one line per document
- each line consists of num_topics many white-space separated floating point numbers
- the rows encode unnormalized probabilities
- each row is an unit vector (ie. all probability is on exactly one topic)
- no two rows assign non-zero probability to the same topic
"""
def test_gamma(test_dir, num_topics, num_documents):
    gamma_path = test_dir + '/' + 'final.gamma'
    f = open(gamma_path, 'r')

    lines = [l.strip() for l in f.readlines()]

    test_passed = True

    if (len(lines) != num_documents):
        print "TEST FAILED: " + gamma_path + " should contain " + str(num_documents) + " lines, one per document."
        test_passed = False
    else:
        unnormalized_topic_per_doc_vecs = [[float(entry) for entry in line.split(' ')] for line in lines]
        if (exists(lambda topic_mix: len(topic_mix) != num_topics, unnormalized_topic_per_doc_vecs)):
            test_passed = False
            print "TEST FAILED: all rows of " + gamma_path + " should contain " + str(num_topics) + " many entries."
            bad_idx = indices_of_satisfiers(lambda topic_mix: len(topic_mix) != num_topics, unnormalized_topic_per_doc_vecs).pop()
            print "Bad row: " + str(lines[bad_idx])
        else:
            topic_per_doc_vecs = map(normalize, unnormalized_topic_per_doc_vecs)
            if (exists(lambda topic_mix: not vector_near_unit(topic_mix, epsilon), topic_per_doc_vecs)):
                test_passed = False
                print "TEST FAILED: all rows of " + gamma_path + " should coutain exactly one entry within " \
                      + str(epsilon) + " of 1 and exactly " + str(num_topics - 1) + \
                      " manys entries within " + str(epsilon) + " of 0.0"
                bad_idx = indices_of_satisfiers(lambda topic_mix: not vector_near_unit(topic_mix, epsilon),
                                                topic_per_doc_vecs).pop()
                print "Example bad probability vector: " + str(topic_per_doc_vecs[bad_idx])
                print "From raw row: " + str(lines[bad_idx])
            else:

                topics_in_docs = union(
                    [indices_of_satisfiers(lambda x: math.fabs(1 - x) < epsilon, dmix) for dmix in topic_per_doc_vecs])
                if (len(topics_in_docs) != num_topics):
                    test_passed = False
                    print "TEST FAILED:  Different documents should have their topic mixes centered on different words."
    f.close()
    return test_passed


if __name__ == "__main__":

    default_test_dir = '/tmp/oni_ldac_test_one_to_one'
    default_epsilon = 0.01
    default_topic_count = 3

    DOC_STRING = """Validates the output of the three topic test: Three docs, three words, three topics, three processes.
    Can LDA handle this trivial situation?
    """

    parser = argparse.ArgumentParser(description=DOC_STRING)
    parser.add_argument("-i", "--input", type=str,
                        help="Path to input directory, relative to user directory. Defaults to " + default_test_dir)

    parser.add_argument("-t", "--topic_count", type=int,
                        help="Number of topics used to generate corpus; will use one unique word per topic. Default:  " \
                             + str(default_topic_count))
    parser.add_argument("-e", "--error_threshold", type=float,
                        help="Error to allow in floating point comparisons. Default:  " \
                             + str(default_epsilon))

    args = parser.parse_args()

    test_dir = args.input if args.input else default_test_dir
    num_topics = args.topic_count if args.topic_count else default_topic_count
    num_words = num_topics
    num_documents = num_topics
    epsilon = args.error_threshold if args.error_threshold else default_epsilon

    test_passed = test_beta(test_dir, num_topics, num_words)
    test_passed &= test_gamma(test_dir, num_topics, num_documents)

    if test_passed:
        print "one_to_one test: PASSED"
        sys.exit(0)
    else:
        print "one_to_one test: FAILED"
        sys.exit(-1)
